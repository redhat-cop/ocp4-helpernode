{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HelperNode v2 The HelperNode is a way to satisfy all the prerequisites needed in order to install OpenShift 4. In version 2 of the HelperNode, all prerequisites/services are run in containers and Ansible is no longer used. For more information about the OpenShift prerequisites. Please see the official OpenShift documentation site The HelperNode is a node/vm/server that sits on your network running all the services needed in an \"all-in-one\" way. The following diagram shows a highlevel view of where the HelperNode sits on your network. NOTE: You can, and is recommended, delegate a subdomain to the HelperNode if you have a working DNS server in your environment. For example; if you want a $CLUSTERID of ocp4 , and you have a $DOMAIN of example.com . Then you will delegate ocp4.example.com to the HelperNode. The HelperNode is meant to help you install an OpenShift 4 cluster using the Platform Agnostic UPI install method (formerly known as the \"BareMetal UPI\" method). It's not meant to do the install for you. This isn't an automation tool, but rather a tool that should be used with your current automation. HelperNode Prerequisites The helpernodectl utility is only (currently) supported/tested with the following: RHEL 8/CentOS 8 Podman 1.6.4 Firewalld 0.8.0 The version of OpenShift is \"tied\" to the binary. For the current version, we are using OpenShift 4.6.12 Quickstarts Please take a look at one of our quickstarts to get up and running. Full Stack Disconnected","title":"Home"},{"location":"#helpernode-v2","text":"The HelperNode is a way to satisfy all the prerequisites needed in order to install OpenShift 4. In version 2 of the HelperNode, all prerequisites/services are run in containers and Ansible is no longer used. For more information about the OpenShift prerequisites. Please see the official OpenShift documentation site The HelperNode is a node/vm/server that sits on your network running all the services needed in an \"all-in-one\" way. The following diagram shows a highlevel view of where the HelperNode sits on your network. NOTE: You can, and is recommended, delegate a subdomain to the HelperNode if you have a working DNS server in your environment. For example; if you want a $CLUSTERID of ocp4 , and you have a $DOMAIN of example.com . Then you will delegate ocp4.example.com to the HelperNode. The HelperNode is meant to help you install an OpenShift 4 cluster using the Platform Agnostic UPI install method (formerly known as the \"BareMetal UPI\" method). It's not meant to do the install for you. This isn't an automation tool, but rather a tool that should be used with your current automation.","title":"HelperNode v2"},{"location":"#helpernode-prerequisites","text":"The helpernodectl utility is only (currently) supported/tested with the following: RHEL 8/CentOS 8 Podman 1.6.4 Firewalld 0.8.0 The version of OpenShift is \"tied\" to the binary. For the current version, we are using OpenShift 4.6.12","title":"HelperNode Prerequisites"},{"location":"#quickstarts","text":"Please take a look at one of our quickstarts to get up and running. Full Stack Disconnected","title":"Quickstarts"},{"location":"binary/","text":"Obtaining The Binary In order to install/use the HelperNode, you'll need to obtain the binary by visiting the releases page on GitHub. Current release is v2alpha2 wget https://github.com/RedHatOfficial/ocp4-helpernode/releases/download/v2alpha2/helpernodectl Copy the binary into your $PATH , and make it executable. sudo cp helpernodectl /usr/local/bin/ sudo chmod +x /usr/local/bin/helpernodectl Global Flags Currently there are two global flags: --config and --log-level Global Flags: -c, --config string config file (default is $HOME/.helpernodectl.yaml) --log-level string log level (e.g. \"debug | info | warn | error\") (default \"info\")","title":"Obtaining The Binary"},{"location":"binary/#obtaining-the-binary","text":"In order to install/use the HelperNode, you'll need to obtain the binary by visiting the releases page on GitHub. Current release is v2alpha2 wget https://github.com/RedHatOfficial/ocp4-helpernode/releases/download/v2alpha2/helpernodectl Copy the binary into your $PATH , and make it executable. sudo cp helpernodectl /usr/local/bin/ sudo chmod +x /usr/local/bin/helpernodectl","title":"Obtaining The Binary"},{"location":"binary/#global-flags","text":"Currently there are two global flags: --config and --log-level Global Flags: -c, --config string config file (default is $HOME/.helpernodectl.yaml) --log-level string log level (e.g. \"debug | info | warn | error\") (default \"info\")","title":"Global Flags"},{"location":"bootstrap-section/","text":"Bootstrap Node Section This section defines the bootstrap node configuration. bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda The options are: bootstrap.name - The hostname (WITHOUT the fqdn) of the bootstrap node you want to set bootstrap.ipaddr - The IP address that you want set (this modifies the dhcp config file, the dns zonefile, and the reverse dns zonefile) bootstrap.macaddr - The mac address for dhcp reservation. This option is not needed if you're doing static ips. bootstrap.disk - The disk that will be used to install RHCOS onto. After an install, you probably want to remove the bootstrap from the loadbalancer. To do this, add bootstrap.inlb and set it to false . For example: bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda inlb: false NOTE You need to stop then start the service after this change has been made.","title":"Bootstrap Node Section"},{"location":"bootstrap-section/#bootstrap-node-section","text":"This section defines the bootstrap node configuration. bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda The options are: bootstrap.name - The hostname (WITHOUT the fqdn) of the bootstrap node you want to set bootstrap.ipaddr - The IP address that you want set (this modifies the dhcp config file, the dns zonefile, and the reverse dns zonefile) bootstrap.macaddr - The mac address for dhcp reservation. This option is not needed if you're doing static ips. bootstrap.disk - The disk that will be used to install RHCOS onto. After an install, you probably want to remove the bootstrap from the loadbalancer. To do this, add bootstrap.inlb and set it to false . For example: bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda inlb: false NOTE You need to stop then start the service after this change has been made.","title":"Bootstrap Node Section"},{"location":"copy-ign/","text":"Copy Ignition Configs This command takes ignition configurations from the given directory, and copies those files into the http contianer. For example: Example: helpernodectl copy-ign --dir=./ocp4/ This command must be run on the host that is to be the helpernode. There is no support for copying the ignition files to an external webserver. Usage: helpernodectl copy-ign --dir /path/to/openshift/install/directory","title":"Copy Ignition Configs"},{"location":"copy-ign/#copy-ignition-configs","text":"This command takes ignition configurations from the given directory, and copies those files into the http contianer. For example: Example: helpernodectl copy-ign --dir=./ocp4/ This command must be run on the host that is to be the helpernode. There is no support for copying the ignition files to an external webserver. Usage: helpernodectl copy-ign --dir /path/to/openshift/install/directory","title":"Copy Ignition Configs"},{"location":"dhcp-section/","text":"DHCP Service Section This section sets up the DHCP server. dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" Explanation of the options you can set: dhcp.router - This is the default gateway of your network you're going to assign to the masters/workers dhcp.bcast - This is the broadcast address for your network dhcp.netmask - This is the netmask that gets assigned to your masters/workers dhcp.poolstart - This is the first address in your dhcp address pool dhcp.poolend - This is the last address in your dhcp address pool dhcp.ipid - This is the ip network id for the range dhcp.netmaskid - This is the networkmask id for the range. DHCP is OPTIONAL if doing static ips. These variables are used to set up the dhcp config file. Note, that you need to also set up the DNS section if using DHCP, even if you're not using the HelperNode DNS server. This is because the DHCP service uses that configuration fo hand out the DNS information via DHCP. Just set the DNS section to whichever DNS server(s) you're using.","title":"DHCP Server Section"},{"location":"dhcp-section/#dhcp-service-section","text":"This section sets up the DHCP server. dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" Explanation of the options you can set: dhcp.router - This is the default gateway of your network you're going to assign to the masters/workers dhcp.bcast - This is the broadcast address for your network dhcp.netmask - This is the netmask that gets assigned to your masters/workers dhcp.poolstart - This is the first address in your dhcp address pool dhcp.poolend - This is the last address in your dhcp address pool dhcp.ipid - This is the ip network id for the range dhcp.netmaskid - This is the networkmask id for the range. DHCP is OPTIONAL if doing static ips. These variables are used to set up the dhcp config file. Note, that you need to also set up the DNS section if using DHCP, even if you're not using the HelperNode DNS server. This is because the DHCP service uses that configuration fo hand out the DNS information via DHCP. Just set the DNS section to whichever DNS server(s) you're using.","title":"DHCP Service Section"},{"location":"disabled-section/","text":"Disabled Services Section By default, the HelperNode CLI utility starts all \"core\" services. If you're only using a subset of services, you can specify the ones you don't want to start. disabledServices: - dhcp - pxe Current list of \"core\" services: pxe http loadbalancer dns dhcp","title":"Disabled Services Section"},{"location":"disabled-section/#disabled-services-section","text":"By default, the HelperNode CLI utility starts all \"core\" services. If you're only using a subset of services, you can specify the ones you don't want to start. disabledServices: - dhcp - pxe Current list of \"core\" services: pxe http loadbalancer dns dhcp","title":"Disabled Services Section"},{"location":"disconnected/","text":"Disconnected Install This quickstart will get the HelperNode running in a disconnected environment. All the HelperNode services are ran in prebuilt image containers, so getting it running disconnected can be broken down into the following steps. Pull the images Save the images into a tarball Sneakernet this tarball over to disconnected network. Load the tarball into a host. Push these images into your hosted registry. Start the HelperNode Services using the internal registry Note: Setting up a regsitry is beyond the scope of this document. Please see Quay or Harbor if you need a registry. You can use the images hosted on your registry for the HelperNode Pull The Images On your laptop, or a system with access to the internet, pull the images. Make sure you're pulling the right tag for the version you want. For the alpha build, we are using latest , but this will change to specific versioning in the future. export TAG=latest for hni in dns dhcp http loadbalancer pxe do podman pull quay.io/helpernode/${hni}:${TAG} done Save The Images After you've pulled the images locally, save them into tarballs, compressing them if you'd like. export TAG=latest for hni in dns dhcp http loadbalancer pxe do podman pull quay.io/helpernode/${hni}:${TAG} podman save --compress -o helpernode-${hni}.tar.gz quay.io/helpernode/${hni}:${TAG} done You can now \"sneakernet\" these helpernode-${hni}.tar.gz files to you Disconnected environment and load it up to a host with access to your image registry. Loading Tarball Load the tarballs into a host on your network with the ability to push into your image registry. for hni in dns dhcp http loadbalancer pxe do podman load -i helpernode-${hni}.tar.gz done You should now have the HelperNode service images on your host. podman images |grep helper The output should look something like this. quay.io/helpernode/pxe latest 26e2169c7c62 5 hours ago 696 MB quay.io/helpernode/loadbalancer latest 600c36a4cdc3 5 hours ago 588 MB quay.io/helpernode/http latest a04dcc414ca6 5 hours ago 1.53 GB quay.io/helpernode/dns latest 1727361b9dfa 5 hours ago 666 MB quay.io/helpernode/dhcp latest b651c044b62e 5 hours ago 592 MB Pushing Images To Registry Now that you have your images locally, you can load them into your images registry. You may need to login first, please consult with your image registry admin. In my example, my registry is registry.example.com on port 5000 # podman login registry.example.com:5000 Username: reguser Password: ********* Login Succeeded! Now I can tag and push my local images to the registry. Note that I'm using a namespace called alpha . You may or maynot have a workspace, if you do; it will probably differ in name. export TAG=latest for hni in dns dhcp http loadbalancer pxe do podman tag quay.io/helpernode/${hni}:${TAG} registry.example.com:5000/alpha/helpernode/${hni}:${TAG} podman push registry.example.com:5000/alpha/helpernode/${hni}:${TAG} done Starting HelperNode Disconnected Once your images are uploaded, you can now start the HelperNode services by first exporting the HELPERNODE_IMAGE_PREFIX environment variable on the HelperNode. export HELPERNODE_IMAGE_PREFIX=registry.example.com:5000/alpha Next, you may need to login to your registry. # podman login registry.example.com:5000 Username: reguser Password: ********* Login Succeeded! Now you can start the service, using a valid YAML config file for your environment. First save the file helpernodectl save -f helpernode.yaml Then start your service helpernodectl start You should now be running the images from your local registry. helpernodectl status The output should look something like this. Names Status Image helpernode-http Up About a minute ago registry.example.com:5000/alpha/helpernode/http:latest helpernode-dhcp Up About a minute ago registry.example.com:5000/alpha/helpernode/dhcp:latest helpernode-dns Up About a minute ago registry.example.com:5000/alpha/helpernode/dns:latest helpernode-pxe Up 2 minutes ago registry.example.com:5000/alpha/helpernode/pxe:latest helpernode-loadbalancer Up 2 minutes ago registry.example.com:5000/alpha/helpernode/loadbalancer:latest OpenShift Installation You can now use the HelperNode to install OpenShift disconnected","title":"Disconnected Installation"},{"location":"disconnected/#disconnected-install","text":"This quickstart will get the HelperNode running in a disconnected environment. All the HelperNode services are ran in prebuilt image containers, so getting it running disconnected can be broken down into the following steps. Pull the images Save the images into a tarball Sneakernet this tarball over to disconnected network. Load the tarball into a host. Push these images into your hosted registry. Start the HelperNode Services using the internal registry Note: Setting up a regsitry is beyond the scope of this document. Please see Quay or Harbor if you need a registry. You can use the images hosted on your registry for the HelperNode","title":"Disconnected Install"},{"location":"disconnected/#pull-the-images","text":"On your laptop, or a system with access to the internet, pull the images. Make sure you're pulling the right tag for the version you want. For the alpha build, we are using latest , but this will change to specific versioning in the future. export TAG=latest for hni in dns dhcp http loadbalancer pxe do podman pull quay.io/helpernode/${hni}:${TAG} done","title":"Pull The Images"},{"location":"disconnected/#save-the-images","text":"After you've pulled the images locally, save them into tarballs, compressing them if you'd like. export TAG=latest for hni in dns dhcp http loadbalancer pxe do podman pull quay.io/helpernode/${hni}:${TAG} podman save --compress -o helpernode-${hni}.tar.gz quay.io/helpernode/${hni}:${TAG} done You can now \"sneakernet\" these helpernode-${hni}.tar.gz files to you Disconnected environment and load it up to a host with access to your image registry.","title":"Save The Images"},{"location":"disconnected/#loading-tarball","text":"Load the tarballs into a host on your network with the ability to push into your image registry. for hni in dns dhcp http loadbalancer pxe do podman load -i helpernode-${hni}.tar.gz done You should now have the HelperNode service images on your host. podman images |grep helper The output should look something like this. quay.io/helpernode/pxe latest 26e2169c7c62 5 hours ago 696 MB quay.io/helpernode/loadbalancer latest 600c36a4cdc3 5 hours ago 588 MB quay.io/helpernode/http latest a04dcc414ca6 5 hours ago 1.53 GB quay.io/helpernode/dns latest 1727361b9dfa 5 hours ago 666 MB quay.io/helpernode/dhcp latest b651c044b62e 5 hours ago 592 MB","title":"Loading Tarball"},{"location":"disconnected/#pushing-images-to-registry","text":"Now that you have your images locally, you can load them into your images registry. You may need to login first, please consult with your image registry admin. In my example, my registry is registry.example.com on port 5000 # podman login registry.example.com:5000 Username: reguser Password: ********* Login Succeeded! Now I can tag and push my local images to the registry. Note that I'm using a namespace called alpha . You may or maynot have a workspace, if you do; it will probably differ in name. export TAG=latest for hni in dns dhcp http loadbalancer pxe do podman tag quay.io/helpernode/${hni}:${TAG} registry.example.com:5000/alpha/helpernode/${hni}:${TAG} podman push registry.example.com:5000/alpha/helpernode/${hni}:${TAG} done","title":"Pushing Images To Registry"},{"location":"disconnected/#starting-helpernode-disconnected","text":"Once your images are uploaded, you can now start the HelperNode services by first exporting the HELPERNODE_IMAGE_PREFIX environment variable on the HelperNode. export HELPERNODE_IMAGE_PREFIX=registry.example.com:5000/alpha Next, you may need to login to your registry. # podman login registry.example.com:5000 Username: reguser Password: ********* Login Succeeded! Now you can start the service, using a valid YAML config file for your environment. First save the file helpernodectl save -f helpernode.yaml Then start your service helpernodectl start You should now be running the images from your local registry. helpernodectl status The output should look something like this. Names Status Image helpernode-http Up About a minute ago registry.example.com:5000/alpha/helpernode/http:latest helpernode-dhcp Up About a minute ago registry.example.com:5000/alpha/helpernode/dhcp:latest helpernode-dns Up About a minute ago registry.example.com:5000/alpha/helpernode/dns:latest helpernode-pxe Up 2 minutes ago registry.example.com:5000/alpha/helpernode/pxe:latest helpernode-loadbalancer Up 2 minutes ago registry.example.com:5000/alpha/helpernode/loadbalancer:latest","title":"Starting HelperNode Disconnected"},{"location":"disconnected/#openshift-installation","text":"You can now use the HelperNode to install OpenShift disconnected","title":"OpenShift Installation"},{"location":"dns-section/","text":"DNS Service Section This section sets up your DNS server. dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" Explanation of the DNS variables: dns.domain - This is what domain the installed DNS server will have. This needs to match what you will put for the baseDomain inside the install-config.yaml OpenShift installer configuration file. dns.clusterid - This is what your clusterid will be named and needs to match what is in metadata.name inside the install-config.yaml file. dns.forwarder1 - This will be set up as the DNS forwarder. This is usually one of the corporate (or \"upstream\") DNS servers. dns.forwarder2 - This will be set up as the second DNS forwarder. This is usually one of the corporate (or \"upstream\") DNS servers. The DNS server will be set up using dns.clusterid + dns.domain as the domain it's serving. In the above example, the helper will be setup to be the SOA for ocp4.example.com . NOTE: Although you CAN use the helper as your dns server. It's best to have your DNS server delegate the dns.clusterid + dns.domain domain to the helper (i.e. Delegate ocp4.example.com to the helper) The DNS section is optional, and only needed if you need to run the DNS server.","title":"DNS Server Section"},{"location":"dns-section/#dns-service-section","text":"This section sets up your DNS server. dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" Explanation of the DNS variables: dns.domain - This is what domain the installed DNS server will have. This needs to match what you will put for the baseDomain inside the install-config.yaml OpenShift installer configuration file. dns.clusterid - This is what your clusterid will be named and needs to match what is in metadata.name inside the install-config.yaml file. dns.forwarder1 - This will be set up as the DNS forwarder. This is usually one of the corporate (or \"upstream\") DNS servers. dns.forwarder2 - This will be set up as the second DNS forwarder. This is usually one of the corporate (or \"upstream\") DNS servers. The DNS server will be set up using dns.clusterid + dns.domain as the domain it's serving. In the above example, the helper will be setup to be the SOA for ocp4.example.com . NOTE: Although you CAN use the helper as your dns server. It's best to have your DNS server delegate the dns.clusterid + dns.domain domain to the helper (i.e. Delegate ocp4.example.com to the helper) The DNS section is optional, and only needed if you need to run the DNS server.","title":"DNS Service Section"},{"location":"fullstack-quickstart/","text":"Full Stack Quickstart This quickstart will get you \"up and running\" with the HelperNode so you can install OpenShift 4. HelperNode v2 will help you install OpenShift using the Platform Agnostic UPI install method. Prerequisites I will be using the following for my HelperNode: CentOS 8 50GB HD 4 vCPUs 8 GB RAM It's important to note that I used the \"Minimal\" install profile. Using this install profiles minimizes the risk of conflicts on the host. The network configuration for this example is as follows: IP - 192.168.7.77 NetMask - 255.255.255.0 Default Gateway - 192.168.7.1 Upstream DNS Servers - 8.8.8.8 , 8.8.4.4 Once the host is installed, you can install the needed packages for the HelperNode. NOTE: The package bash-completion is optional but helpful. yum -y install bash-completion podman Next, install the helpernodectl CLI utility binary and put it somehwere in your $PATH , in my example I'm installing it in /usr/local/bin curl -LO https://github.com/RedHatOfficial/ocp4-helpernode/releases/download/v2alpha2/helpernodectl mv helpernodectl /usr/local/bin/ chmod +x /usr/local/bin/helpernodectl If you don't have /usr/local/bin in your $PATH , you can export it with the following command. export PATH=/usr/local/bin:$PATH Enable bash completion source <(helpernodectl completion bash) Setting Up The YAML File You will need to set up your YAML file for your environment. This includes things like knowing the MAC Addresses of the OpenShift nodes that will be installed (either VMs or Bare Metal hosts) and other infrastructure related things. Please consult the YAML File Documentation for more details. As I'm going to do a \"Full Stack\" install (using all services). I will use the following YAML file and save it as helpernode.yaml . version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda It's important to note that helper.ipaddr and helper.networkifacename are found using the ip addr command. # ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 52:54:00:53:9c:c9 brd ff:ff:ff:ff:ff:ff inet 192.168.7.77/24 brd 192.168.7.255 scope global noprefixroute ens3 valid_lft forever preferred_lft forever inet6 fe80::ca70:e130:a9dd:71e8/64 scope link noprefixroute valid_lft forever preferred_lft forever Saving The Configuration Although it's possible to pass --config to the helpernodectl command, it's better to save it. This saves you from having to pass --config to each subcommand. helpernodectl save -f helpernode.yaml This will save the file under ~/.helper.yaml . NOTE : Do not modify ~/.helper.yaml directly. The correct way to modify is to edit your YAML ( helpernode.yaml for example) and rerun the save subcommand. Preflight Checks Now that you have the binary, perform preflight checks to see if there are any potential issues. helpernodectl preflight The output should look like this INFO[0000] RUNNING PREFLIGHT TASKS INFO[0000] Running firewall checks INFO[0001] Preflight checks for Firewall Firewall Issues=10 INFO[0001] Starting Port Checks INFO[0001] Preflight checks for Ports Port Issues=0 INFO[0001] Starting Systemd Checks INFO[0001] Preflight checks for Systemd Systemd Issues=0 INFO[0001] Preflight Summary FWRules=10 PortCheck=0 SystemdCheck=0 FATA[0001] Cannot Start, preflight errors found This shows that I have 10 potential firewall rules that are not in place. Preflight also checks for port conflicts (in case you're running other services, NOT recomended btw) and Systemd conflicts as well. At this point we recommend the users to handle these manually. For more information about what ports are needed, consult the preflight cli documentation. There is an experimental flag --fix-all that tries to add firewall rules and disable conflicting systemd service. helpernodectl preflight --fix-all The output of --fix-all should look like this: INFO[0000] RUNNING PREFLIGHT TASKS INFO[0000] ==========================BESTEFFORT IN FIXING ERRORS============================ INFO[0000] Running firewall checks INFO[0001] OPENING PORT: 6443/tcp INFO[0002] OPENING PORT: 22623/tcp INFO[0003] OPENING PORT: 8080/tcp INFO[0004] OPENING PORT: 9000/tcp INFO[0005] OPENING PORT: 67/udp INFO[0006] OPENING PORT: 53/tcp INFO[0007] OPENING PORT: 53/udp INFO[0008] OPENING PORT: 80/tcp INFO[0009] OPENING PORT: 443/tcp INFO[0010] OPENING PORT: 69/udp INFO[0011] Preflight checks for Firewall Firewall Issues=10 Firewall rules added=10 INFO[0011] Starting Port Checks INFO[0011] Preflight checks for Ports Port Issues=0 INFO[0011] Starting Systemd Checks INFO[0011] Preflight checks for Systemd Systemd Issues=0 INFO[0011] Preflight Summary FWRules=10 PortCheck=0 SystemdCheck=0 Note, use with caution. Once the cli reports No preflight errors found , you can proceed. helpernodectl preflight Output should say \"No preflight errors found\": INFO[0000] RUNNING PREFLIGHT TASKS INFO[0000] Running firewall checks INFO[0001] Preflight checks for Firewall Firewall Issues=0 INFO[0001] Starting Port Checks INFO[0001] Preflight checks for Ports Port Issues=0 INFO[0001] Starting Systemd Checks INFO[0001] Preflight checks for Systemd Systemd Issues=0 INFO[0001] Preflight Summary FWRules=0 PortCheck=0 SystemdCheck=0 INFO[0001] No preflight errors found Prepulling Images You can, optionally, prepull images to make startup faster helpernodectl pull This isn't required, as the start command will pull them anyway, but will speed up startup. Starting Services Once I have my YAML file ready, saved, and images pulled, I can start the HelperNode services. helpernodectl start This should run preflight and start the services: INFO[0001] Found a configuration INFO[0001] RUNNING PREFLIGHT TASKS INFO[0001] Running firewall checks INFO[0002] Preflight checks for Firewall Firewall Issues=0 INFO[0002] Starting Port Checks INFO[0002] Preflight checks for Ports Port Issues=0 INFO[0002] Starting Systemd Checks INFO[0002] Preflight checks for Systemd Systemd Issues=0 INFO[0002] Preflight Summary FWRules=0 PortCheck=0 SystemdCheck=0 INFO[0002] No preflight errors found INFO[0002] Starting Containers ====================== INFO[0002] Validationg configuration in helpernode.yaml INFO[0002] Starting helpernode-dns INFO[0002] Validationg configuration in helpernode.yaml INFO[0002] Starting helpernode-dhcp INFO[0003] Validationg configuration in helpernode.yaml INFO[0003] Starting helpernode-http INFO[0004] Validationg configuration in helpernode.yaml INFO[0004] Starting helpernode-loadbalancer INFO[0005] Validationg configuration in helpernode.yaml INFO[0005] Starting helpernode-pxe Note that a preflight check is done as part of the starting process. You can skip these checks (to speed up startup, if you've already done preflight) by passing the --skip-preflight option. Once startup as finsihed, you can check to see if it's running. helpernodectl status This should output the running services: Names Status Image helpernode-pxe Up 38 seconds ago quay.io/helpernode/pxe:latest helpernode-loadbalancer Up 40 seconds ago quay.io/helpernode/loadbalancer:latest helpernode-http Up 41 seconds ago quay.io/helpernode/http:latest helpernode-dhcp Up 42 seconds ago quay.io/helpernode/dhcp:latest helpernode-dns Up 42 seconds ago quay.io/helpernode/dns:latest Obtaining The OCP Clients In order to install OpenShift, I will need the clients. The HelperNode cli utility can fetch them for you by simply running the get-clients command helpernodectl get-clients This should output the following INFO[0000] Getting file openshift-client-linux.tar.gz INFO[0000] Getting file openshift-install-linux.tar.gz INFO[0000] Getting file helm.tar.gz This command fetches the clients into your current working directory. ls -1 *.gz This should output 3 tarballs: helm.tar.gz openshift-client-linux.tar.gz openshift-install-linux.tar.gz Go ahead and extract them into your $PATH ; I did mine on /usr/local/bin tar -xzf openshift-client-linux.tar.gz -C /usr/local/bin/ tar -xzf openshift-install-linux.tar.gz -C /usr/local/bin/ tar -xzf helm.tar.gz mv linux-amd64/helm /usr/local/bin/ rm -f /usr/local/bin/README.md You should now have oc , openshift-install , kubectl , and helm in your path ls -l /usr/local/bin/{oc,openshift-install,kubectl,helm} This should list all the binaries that were extracted: -rwxr-xr-x. 1 3434 3434 41607168 Oct 26 08:36 /usr/local/bin/helm -rwxr-xr-x. 2 root root 74528264 Nov 4 13:38 /usr/local/bin/kubectl -rwxr-xr-x. 2 root root 74528264 Nov 4 13:38 /usr/local/bin/oc -rwxr-xr-x. 1 root root 353038336 Nov 6 04:41 /usr/local/bin/openshift-install Installing OpenShift I will take you installing OpenShift at a HIGH level. Please visit the Official OpenShift Installation Documentation Site for more specific details about installing OpenShift. In order to install OpenShift you will need an ssh key. If you don't have one, create one: ssh-keygen You'll also need a place to store your pull secret. I use ~/.openshift as the location: mkdir -p ~/.openshift Visit cloud.redhat.com and select \"Bare Metal\" then \"UPI\". Download your pull secret and save it under ~/.openshift/pull-secret ls -1 ~/.openshift/pull-secret This should list your pullsecret /root/.openshift/pull-secret Now create an install directory for the openshift-install command to use. mkdir ~/ocp4 cd ~/ocp4 Next, create an install-config.yaml file. Here is a sample for my environment: NOTE: Make sure to change for your environment where applicable. cat <<EOF > install-config.yaml apiVersion: v1 baseDomain: example.com compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp4 networking: clusterNetworks: - cidr: 10.254.0.0/16 hostPrefix: 24 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} pullSecret: '$(< ~/.openshift/pull-secret)' sshKey: '$(< ~/.ssh/id_rsa.pub)' EOF Next, create the OpenShift install manifest. openshift-install create manifests This should output the following: INFO Consuming Install Config from target directory WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings INFO Manifests created in: manifests and openshift Edit the manifests/cluster-scheduler-02-config.yml Kubernetes manifest file to prevent Pods from being scheduled on the control plane machines by setting mastersSchedulable to false. NOTE : Skip this step if you're installing a compact cluster! sed -i 's/mastersSchedulable: true/mastersSchedulable: false/g' manifests/cluster-scheduler-02-config.yml View the file for varification. cat manifests/cluster-scheduler-02-config.yml The output should look something like this: apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false policy: name: \"\" status: {} Next, generate the ignition configs openshift-install create ignition-configs Now you need copy these ignition configs to the http HelperNode service. You do this by running the copy-ign command. NOTE: You need to pass --dir and the path where the ignition files are located. helpernodectl copy-ign --dir ~/ocp4/ Output should look something like this INFO[0000] Copying over /root/ocp4/bootstrap.ign to http container INFO[0000] Copying over /root/ocp4/master.ign to http container INFO[0000] Copying over /root/ocp4/worker.ign to http container Install RHCOS PXE boot into your Instances and they should load up the right configuration based on the MAC address. The DHCP server is set up with MAC address filtering and the PXE service is configured to load the right config to the right machine (based on mac address). Boot/install the VMs/Instances in the following order Bootstrap Masters Workers On your laptop/workstation visit the status page firefox http://192.168.7.77:9000 NOTE: Make sure you don't expose this port in public cloud environments! You'll see the bootstrap turn \"green\" and then the masters turn \"green\", then the bootstrap turn \"red\". This is your indication that you can continue. You need to make sure your DNS on the node is setup correctly. You'll need to point to localhost (or 127.0.0.1 ) in order to connect to your cluster. This is what I have in my /etc/resolv.conf cat /etc/resolv.conf This is a sample output: # Generated by NetworkManager nameserver 127.0.0.1 nameserver 8.8.8.8 Wait For Install The boostrap VM actually does the install for you; you can track it with the following command. openshift-install wait-for bootstrap-complete --log-level debug Once you see this message below... DEBUG OpenShift Installer 4.6.4 DEBUG Built from commit 6e02d049701437fa81521fe981405745a62c86c5 INFO Waiting up to 20m0s for the Kubernetes API at https://api.ocp4.example.com:6443... INFO API v1.19.0+9f84db3 up INFO Waiting up to 30m0s for bootstrapping to complete... DEBUG Bootstrap status: complete INFO It is now safe to remove the bootstrap resources DEBUG Time elapsed per stage: DEBUG Bootstrap Complete: 6m42s INFO Time elapsed: 6m42s ...you can continue...at this point you can delete/poweroff the bootstrap server. NOTE: You can repourpose this machine as another node! Finish Install First, login to your cluster export KUBECONFIG=/root/ocp4/auth/kubeconfig Your install may be waiting for worker nodes to get approved. Normally the machineconfig node approval operator takes care of this for you. However, in a Platform Agnostic UPI install, this needs to be done manually. Check pending CSRs with the following command. oc get csr You can approve all pending CSRs in \"one shot\" with the following oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve You may have to run this multiple times depending on how many workers you have and in what order they come in. Keep a watch on these CSRs watch oc get csr In order to setup your registry, you first have to set the managementState to Managed for your cluster oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}' For PoCs, using emptyDir is okay (to use PVs follow this doc) oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"storage\":{\"emptyDir\":{}}}}' If you need to expose the registry, run this command oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p '{\"spec\":{\"defaultRoute\":true}}' To finish the install process, run the following openshift-install wait-for install-complete Note: You can watch the operators running with oc get clusteroperators in another window with a watch to see it progress Login to the web console The OpenShift 4 web console will be running at https://console-openshift-console.apps.{{ dns.clusterid }}.{{ dns.domain }} (e.g. https://console-openshift-console.apps.ocp4.example.com ) Username: kubeadmin Password: the output of cat /root/ocp4/auth/kubeadmin-password Upgrade If you didn't install the latest release, then just run the following to upgrade. oc adm upgrade --to-latest Scale the router if you need to (if, for example, you installed a compact cluster or are using 3 workers) oc patch --namespace=openshift-ingress-operator --patch='{\"spec\": {\"replicas\": 3}}' --type=merge ingresscontroller/default DONE Your cluster should be ready to use!","title":"Full Stack Quickstart"},{"location":"fullstack-quickstart/#full-stack-quickstart","text":"This quickstart will get you \"up and running\" with the HelperNode so you can install OpenShift 4. HelperNode v2 will help you install OpenShift using the Platform Agnostic UPI install method.","title":"Full Stack Quickstart"},{"location":"fullstack-quickstart/#prerequisites","text":"I will be using the following for my HelperNode: CentOS 8 50GB HD 4 vCPUs 8 GB RAM It's important to note that I used the \"Minimal\" install profile. Using this install profiles minimizes the risk of conflicts on the host. The network configuration for this example is as follows: IP - 192.168.7.77 NetMask - 255.255.255.0 Default Gateway - 192.168.7.1 Upstream DNS Servers - 8.8.8.8 , 8.8.4.4 Once the host is installed, you can install the needed packages for the HelperNode. NOTE: The package bash-completion is optional but helpful. yum -y install bash-completion podman Next, install the helpernodectl CLI utility binary and put it somehwere in your $PATH , in my example I'm installing it in /usr/local/bin curl -LO https://github.com/RedHatOfficial/ocp4-helpernode/releases/download/v2alpha2/helpernodectl mv helpernodectl /usr/local/bin/ chmod +x /usr/local/bin/helpernodectl If you don't have /usr/local/bin in your $PATH , you can export it with the following command. export PATH=/usr/local/bin:$PATH Enable bash completion source <(helpernodectl completion bash)","title":"Prerequisites"},{"location":"fullstack-quickstart/#setting-up-the-yaml-file","text":"You will need to set up your YAML file for your environment. This includes things like knowing the MAC Addresses of the OpenShift nodes that will be installed (either VMs or Bare Metal hosts) and other infrastructure related things. Please consult the YAML File Documentation for more details. As I'm going to do a \"Full Stack\" install (using all services). I will use the following YAML file and save it as helpernode.yaml . version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda It's important to note that helper.ipaddr and helper.networkifacename are found using the ip addr command. # ip addr 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 52:54:00:53:9c:c9 brd ff:ff:ff:ff:ff:ff inet 192.168.7.77/24 brd 192.168.7.255 scope global noprefixroute ens3 valid_lft forever preferred_lft forever inet6 fe80::ca70:e130:a9dd:71e8/64 scope link noprefixroute valid_lft forever preferred_lft forever","title":"Setting Up The YAML File"},{"location":"fullstack-quickstart/#saving-the-configuration","text":"Although it's possible to pass --config to the helpernodectl command, it's better to save it. This saves you from having to pass --config to each subcommand. helpernodectl save -f helpernode.yaml This will save the file under ~/.helper.yaml . NOTE : Do not modify ~/.helper.yaml directly. The correct way to modify is to edit your YAML ( helpernode.yaml for example) and rerun the save subcommand.","title":"Saving The Configuration"},{"location":"fullstack-quickstart/#preflight-checks","text":"Now that you have the binary, perform preflight checks to see if there are any potential issues. helpernodectl preflight The output should look like this INFO[0000] RUNNING PREFLIGHT TASKS INFO[0000] Running firewall checks INFO[0001] Preflight checks for Firewall Firewall Issues=10 INFO[0001] Starting Port Checks INFO[0001] Preflight checks for Ports Port Issues=0 INFO[0001] Starting Systemd Checks INFO[0001] Preflight checks for Systemd Systemd Issues=0 INFO[0001] Preflight Summary FWRules=10 PortCheck=0 SystemdCheck=0 FATA[0001] Cannot Start, preflight errors found This shows that I have 10 potential firewall rules that are not in place. Preflight also checks for port conflicts (in case you're running other services, NOT recomended btw) and Systemd conflicts as well. At this point we recommend the users to handle these manually. For more information about what ports are needed, consult the preflight cli documentation. There is an experimental flag --fix-all that tries to add firewall rules and disable conflicting systemd service. helpernodectl preflight --fix-all The output of --fix-all should look like this: INFO[0000] RUNNING PREFLIGHT TASKS INFO[0000] ==========================BESTEFFORT IN FIXING ERRORS============================ INFO[0000] Running firewall checks INFO[0001] OPENING PORT: 6443/tcp INFO[0002] OPENING PORT: 22623/tcp INFO[0003] OPENING PORT: 8080/tcp INFO[0004] OPENING PORT: 9000/tcp INFO[0005] OPENING PORT: 67/udp INFO[0006] OPENING PORT: 53/tcp INFO[0007] OPENING PORT: 53/udp INFO[0008] OPENING PORT: 80/tcp INFO[0009] OPENING PORT: 443/tcp INFO[0010] OPENING PORT: 69/udp INFO[0011] Preflight checks for Firewall Firewall Issues=10 Firewall rules added=10 INFO[0011] Starting Port Checks INFO[0011] Preflight checks for Ports Port Issues=0 INFO[0011] Starting Systemd Checks INFO[0011] Preflight checks for Systemd Systemd Issues=0 INFO[0011] Preflight Summary FWRules=10 PortCheck=0 SystemdCheck=0 Note, use with caution. Once the cli reports No preflight errors found , you can proceed. helpernodectl preflight Output should say \"No preflight errors found\": INFO[0000] RUNNING PREFLIGHT TASKS INFO[0000] Running firewall checks INFO[0001] Preflight checks for Firewall Firewall Issues=0 INFO[0001] Starting Port Checks INFO[0001] Preflight checks for Ports Port Issues=0 INFO[0001] Starting Systemd Checks INFO[0001] Preflight checks for Systemd Systemd Issues=0 INFO[0001] Preflight Summary FWRules=0 PortCheck=0 SystemdCheck=0 INFO[0001] No preflight errors found","title":"Preflight Checks"},{"location":"fullstack-quickstart/#prepulling-images","text":"You can, optionally, prepull images to make startup faster helpernodectl pull This isn't required, as the start command will pull them anyway, but will speed up startup.","title":"Prepulling Images"},{"location":"fullstack-quickstart/#starting-services","text":"Once I have my YAML file ready, saved, and images pulled, I can start the HelperNode services. helpernodectl start This should run preflight and start the services: INFO[0001] Found a configuration INFO[0001] RUNNING PREFLIGHT TASKS INFO[0001] Running firewall checks INFO[0002] Preflight checks for Firewall Firewall Issues=0 INFO[0002] Starting Port Checks INFO[0002] Preflight checks for Ports Port Issues=0 INFO[0002] Starting Systemd Checks INFO[0002] Preflight checks for Systemd Systemd Issues=0 INFO[0002] Preflight Summary FWRules=0 PortCheck=0 SystemdCheck=0 INFO[0002] No preflight errors found INFO[0002] Starting Containers ====================== INFO[0002] Validationg configuration in helpernode.yaml INFO[0002] Starting helpernode-dns INFO[0002] Validationg configuration in helpernode.yaml INFO[0002] Starting helpernode-dhcp INFO[0003] Validationg configuration in helpernode.yaml INFO[0003] Starting helpernode-http INFO[0004] Validationg configuration in helpernode.yaml INFO[0004] Starting helpernode-loadbalancer INFO[0005] Validationg configuration in helpernode.yaml INFO[0005] Starting helpernode-pxe Note that a preflight check is done as part of the starting process. You can skip these checks (to speed up startup, if you've already done preflight) by passing the --skip-preflight option. Once startup as finsihed, you can check to see if it's running. helpernodectl status This should output the running services: Names Status Image helpernode-pxe Up 38 seconds ago quay.io/helpernode/pxe:latest helpernode-loadbalancer Up 40 seconds ago quay.io/helpernode/loadbalancer:latest helpernode-http Up 41 seconds ago quay.io/helpernode/http:latest helpernode-dhcp Up 42 seconds ago quay.io/helpernode/dhcp:latest helpernode-dns Up 42 seconds ago quay.io/helpernode/dns:latest","title":"Starting Services"},{"location":"fullstack-quickstart/#obtaining-the-ocp-clients","text":"In order to install OpenShift, I will need the clients. The HelperNode cli utility can fetch them for you by simply running the get-clients command helpernodectl get-clients This should output the following INFO[0000] Getting file openshift-client-linux.tar.gz INFO[0000] Getting file openshift-install-linux.tar.gz INFO[0000] Getting file helm.tar.gz This command fetches the clients into your current working directory. ls -1 *.gz This should output 3 tarballs: helm.tar.gz openshift-client-linux.tar.gz openshift-install-linux.tar.gz Go ahead and extract them into your $PATH ; I did mine on /usr/local/bin tar -xzf openshift-client-linux.tar.gz -C /usr/local/bin/ tar -xzf openshift-install-linux.tar.gz -C /usr/local/bin/ tar -xzf helm.tar.gz mv linux-amd64/helm /usr/local/bin/ rm -f /usr/local/bin/README.md You should now have oc , openshift-install , kubectl , and helm in your path ls -l /usr/local/bin/{oc,openshift-install,kubectl,helm} This should list all the binaries that were extracted: -rwxr-xr-x. 1 3434 3434 41607168 Oct 26 08:36 /usr/local/bin/helm -rwxr-xr-x. 2 root root 74528264 Nov 4 13:38 /usr/local/bin/kubectl -rwxr-xr-x. 2 root root 74528264 Nov 4 13:38 /usr/local/bin/oc -rwxr-xr-x. 1 root root 353038336 Nov 6 04:41 /usr/local/bin/openshift-install","title":"Obtaining The OCP Clients"},{"location":"fullstack-quickstart/#installing-openshift","text":"I will take you installing OpenShift at a HIGH level. Please visit the Official OpenShift Installation Documentation Site for more specific details about installing OpenShift. In order to install OpenShift you will need an ssh key. If you don't have one, create one: ssh-keygen You'll also need a place to store your pull secret. I use ~/.openshift as the location: mkdir -p ~/.openshift Visit cloud.redhat.com and select \"Bare Metal\" then \"UPI\". Download your pull secret and save it under ~/.openshift/pull-secret ls -1 ~/.openshift/pull-secret This should list your pullsecret /root/.openshift/pull-secret Now create an install directory for the openshift-install command to use. mkdir ~/ocp4 cd ~/ocp4 Next, create an install-config.yaml file. Here is a sample for my environment: NOTE: Make sure to change for your environment where applicable. cat <<EOF > install-config.yaml apiVersion: v1 baseDomain: example.com compute: - hyperthreading: Enabled name: worker replicas: 0 controlPlane: hyperthreading: Enabled name: master replicas: 3 metadata: name: ocp4 networking: clusterNetworks: - cidr: 10.254.0.0/16 hostPrefix: 24 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: none: {} pullSecret: '$(< ~/.openshift/pull-secret)' sshKey: '$(< ~/.ssh/id_rsa.pub)' EOF Next, create the OpenShift install manifest. openshift-install create manifests This should output the following: INFO Consuming Install Config from target directory WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings INFO Manifests created in: manifests and openshift Edit the manifests/cluster-scheduler-02-config.yml Kubernetes manifest file to prevent Pods from being scheduled on the control plane machines by setting mastersSchedulable to false. NOTE : Skip this step if you're installing a compact cluster! sed -i 's/mastersSchedulable: true/mastersSchedulable: false/g' manifests/cluster-scheduler-02-config.yml View the file for varification. cat manifests/cluster-scheduler-02-config.yml The output should look something like this: apiVersion: config.openshift.io/v1 kind: Scheduler metadata: creationTimestamp: null name: cluster spec: mastersSchedulable: false policy: name: \"\" status: {} Next, generate the ignition configs openshift-install create ignition-configs Now you need copy these ignition configs to the http HelperNode service. You do this by running the copy-ign command. NOTE: You need to pass --dir and the path where the ignition files are located. helpernodectl copy-ign --dir ~/ocp4/ Output should look something like this INFO[0000] Copying over /root/ocp4/bootstrap.ign to http container INFO[0000] Copying over /root/ocp4/master.ign to http container INFO[0000] Copying over /root/ocp4/worker.ign to http container","title":"Installing OpenShift"},{"location":"fullstack-quickstart/#install-rhcos","text":"PXE boot into your Instances and they should load up the right configuration based on the MAC address. The DHCP server is set up with MAC address filtering and the PXE service is configured to load the right config to the right machine (based on mac address). Boot/install the VMs/Instances in the following order Bootstrap Masters Workers On your laptop/workstation visit the status page firefox http://192.168.7.77:9000 NOTE: Make sure you don't expose this port in public cloud environments! You'll see the bootstrap turn \"green\" and then the masters turn \"green\", then the bootstrap turn \"red\". This is your indication that you can continue. You need to make sure your DNS on the node is setup correctly. You'll need to point to localhost (or 127.0.0.1 ) in order to connect to your cluster. This is what I have in my /etc/resolv.conf cat /etc/resolv.conf This is a sample output: # Generated by NetworkManager nameserver 127.0.0.1 nameserver 8.8.8.8","title":"Install RHCOS"},{"location":"fullstack-quickstart/#wait-for-install","text":"The boostrap VM actually does the install for you; you can track it with the following command. openshift-install wait-for bootstrap-complete --log-level debug Once you see this message below... DEBUG OpenShift Installer 4.6.4 DEBUG Built from commit 6e02d049701437fa81521fe981405745a62c86c5 INFO Waiting up to 20m0s for the Kubernetes API at https://api.ocp4.example.com:6443... INFO API v1.19.0+9f84db3 up INFO Waiting up to 30m0s for bootstrapping to complete... DEBUG Bootstrap status: complete INFO It is now safe to remove the bootstrap resources DEBUG Time elapsed per stage: DEBUG Bootstrap Complete: 6m42s INFO Time elapsed: 6m42s ...you can continue...at this point you can delete/poweroff the bootstrap server. NOTE: You can repourpose this machine as another node!","title":"Wait For Install"},{"location":"fullstack-quickstart/#finish-install","text":"First, login to your cluster export KUBECONFIG=/root/ocp4/auth/kubeconfig Your install may be waiting for worker nodes to get approved. Normally the machineconfig node approval operator takes care of this for you. However, in a Platform Agnostic UPI install, this needs to be done manually. Check pending CSRs with the following command. oc get csr You can approve all pending CSRs in \"one shot\" with the following oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{\"\\n\"}}{{end}}{{end}}' | xargs oc adm certificate approve You may have to run this multiple times depending on how many workers you have and in what order they come in. Keep a watch on these CSRs watch oc get csr In order to setup your registry, you first have to set the managementState to Managed for your cluster oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}' For PoCs, using emptyDir is okay (to use PVs follow this doc) oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"storage\":{\"emptyDir\":{}}}}' If you need to expose the registry, run this command oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p '{\"spec\":{\"defaultRoute\":true}}' To finish the install process, run the following openshift-install wait-for install-complete Note: You can watch the operators running with oc get clusteroperators in another window with a watch to see it progress","title":"Finish Install"},{"location":"fullstack-quickstart/#login-to-the-web-console","text":"The OpenShift 4 web console will be running at https://console-openshift-console.apps.{{ dns.clusterid }}.{{ dns.domain }} (e.g. https://console-openshift-console.apps.ocp4.example.com ) Username: kubeadmin Password: the output of cat /root/ocp4/auth/kubeadmin-password","title":"Login to the web console"},{"location":"fullstack-quickstart/#upgrade","text":"If you didn't install the latest release, then just run the following to upgrade. oc adm upgrade --to-latest Scale the router if you need to (if, for example, you installed a compact cluster or are using 3 workers) oc patch --namespace=openshift-ingress-operator --patch='{\"spec\": {\"replicas\": 3}}' --type=merge ingresscontroller/default","title":"Upgrade"},{"location":"fullstack-quickstart/#done","text":"Your cluster should be ready to use!","title":"DONE"},{"location":"get-clients/","text":"Get Clients This will get the needed clients from the holding container. Which is, by default, the http container. It saves these in your current working directory. Example: helpernodectl get-clients Currently, the clients that are provided are: helm kubectl oc openshift-install These are provided via a compressed tarball (i.e. tar.gz ). It is left to the user to extract them in the proper $PATH location.","title":"Get Clients"},{"location":"get-clients/#get-clients","text":"This will get the needed clients from the holding container. Which is, by default, the http container. It saves these in your current working directory. Example: helpernodectl get-clients Currently, the clients that are provided are: helm kubectl oc openshift-install These are provided via a compressed tarball (i.e. tar.gz ). It is left to the user to extract them in the proper $PATH location.","title":"Get Clients"},{"location":"helper-section/","text":"Helper Section This section sets up the variables for the server/vm/node where the HelperNode services will run. helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" This is how it breaks down helper.name - REQUIRED: This needs to be set to the hostname you want your helper to be (some people leave it as \"helper\" others change it to \"bastion\"). This will create an entry in the DNS service. helper.ipaddr - REQUIRED Set this to the current IP address of the helper. This is used to set up the reverse dns definition of not only the HelperNode itself, but also the in-addr.arpa configuration in DNS. helper.networkifacename - REQUIRED: This is set to the interface that has the helper.ipaddr ip address. NOTE: The helper.networkifacename is the ACTUAL name of the interface, NOT the NetworkManager name (you should NEVER need to set it to something like System eth0 . Set it to what you see when you run the ip addr command)","title":"Helper Section"},{"location":"helper-section/#helper-section","text":"This section sets up the variables for the server/vm/node where the HelperNode services will run. helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" This is how it breaks down helper.name - REQUIRED: This needs to be set to the hostname you want your helper to be (some people leave it as \"helper\" others change it to \"bastion\"). This will create an entry in the DNS service. helper.ipaddr - REQUIRED Set this to the current IP address of the helper. This is used to set up the reverse dns definition of not only the HelperNode itself, but also the in-addr.arpa configuration in DNS. helper.networkifacename - REQUIRED: This is set to the interface that has the helper.ipaddr ip address. NOTE: The helper.networkifacename is the ACTUAL name of the interface, NOT the NetworkManager name (you should NEVER need to set it to something like System eth0 . Set it to what you see when you run the ip addr command)","title":"Helper Section"},{"location":"ipi-specific/","text":"Disabled Services Section By default, the HelperNode services assumes you will use the Load Balancer functionality. In the case of BareMetal IPI, vSphere IPI, OpenStack IPI, and RHV IPI; the load balancing is done on the platform itself. You can flag this in the YAML file. ipiip: api: \"192.168.7.200\" apps: \"192.168.7.201\" NOTE: These IPs must NOT be in use, as they'll be assigned by the OpenShift installer itself. Please see the official documentation for more details. Doing this, you'll no longer need the loadbalancer service. You can disable this in the YAML as well. disabledServices: - loadbalancer You can also disable pxe as most IPI installers don't need it. This depends on the IPI implementation. Again, please consult the official documentation.","title":"IPI Installation Section"},{"location":"ipi-specific/#disabled-services-section","text":"By default, the HelperNode services assumes you will use the Load Balancer functionality. In the case of BareMetal IPI, vSphere IPI, OpenStack IPI, and RHV IPI; the load balancing is done on the platform itself. You can flag this in the YAML file. ipiip: api: \"192.168.7.200\" apps: \"192.168.7.201\" NOTE: These IPs must NOT be in use, as they'll be assigned by the OpenShift installer itself. Please see the official documentation for more details. Doing this, you'll no longer need the loadbalancer service. You can disable this in the YAML as well. disabledServices: - loadbalancer You can also disable pxe as most IPI installers don't need it. This depends on the IPI implementation. Again, please consult the official documentation.","title":"Disabled Services Section"},{"location":"master-section/","text":"Master Nodes Section Similar to the bootstrap section; this sets up master node configuration. Please note that this is an array. masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda masters.name - The hostname (WITHOUT the fqdn) of the master node you want to set (x of 3). masters.ipaddr - The IP address (x of 3) that you want set (this modifies the dhcp config file, the dns zonefile, and the reverse dns zonefile) masters.macaddr - The mac address for dhcp reservation. This option is not needed if you're doing static ips. masters.disk - The name of the disk to install RHCOS onto. NOTE: 3 Masters are MANDATORY for installation of OpenShift 4","title":"Master Nodes Section"},{"location":"master-section/#master-nodes-section","text":"Similar to the bootstrap section; this sets up master node configuration. Please note that this is an array. masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda masters.name - The hostname (WITHOUT the fqdn) of the master node you want to set (x of 3). masters.ipaddr - The IP address (x of 3) that you want set (this modifies the dhcp config file, the dns zonefile, and the reverse dns zonefile) masters.macaddr - The mac address for dhcp reservation. This option is not needed if you're doing static ips. masters.disk - The name of the disk to install RHCOS onto. NOTE: 3 Masters are MANDATORY for installation of OpenShift 4","title":"Master Nodes Section"},{"location":"pluggable-services/","text":"Pluggable Services You are able to launch other containers that are not part of the \"core\" container group. This can be done by adding pluggableService to your config. Below is an example: pluggableServices: anotherweb: image: quay.io/christianh814/test-webserver:latest ports: - 8899/tcp - 8899/udp myweb: image: quay.io/christianh814/webserver-test:latest ports: - 8888/tcp You will need: pluggableServices.<name> - Name your service something. You must name it something not in the \"reserved names\". For example you CANNOT name your service loadbalancer as that's reserved. pluggableServices.image - This is the registry where the image is stored. You must pass the tag as well. pluggableServices.ports - This is an array of ports with their protocol. NOTE : Binding to local storage is not supported currently, but it is planned.","title":"Pluggable Services"},{"location":"pluggable-services/#pluggable-services","text":"You are able to launch other containers that are not part of the \"core\" container group. This can be done by adding pluggableService to your config. Below is an example: pluggableServices: anotherweb: image: quay.io/christianh814/test-webserver:latest ports: - 8899/tcp - 8899/udp myweb: image: quay.io/christianh814/webserver-test:latest ports: - 8888/tcp You will need: pluggableServices.<name> - Name your service something. You must name it something not in the \"reserved names\". For example you CANNOT name your service loadbalancer as that's reserved. pluggableServices.image - This is the registry where the image is stored. You must pass the tag as well. pluggableServices.ports - This is an array of ports with their protocol. NOTE : Binding to local storage is not supported currently, but it is planned.","title":"Pluggable Services"},{"location":"preflight/","text":"Preflight The preflight subcommand checks for port conflicts, systemd/service conflicts, and firewall rules on the host and can optionally fix errors it finds. Example: helpernodectl preflight This will only report conflicts/errors and leaves it up to the user to fix/reconcile. You can optionally fix systemd and firewall rules by passing the --fix-all flag (EXPERIMENTAL). Example: helpernodectl preflight --fix-all Again, this is experimental and assumes you have firewalld running on a RHEL 8/CentOS 8 server/node/machine/vm. Currently the firewall rules needed for the HelperNode are: 6443/tcp 22623/tcp 8080/tcp 9000/tcp 9090/tcp 67/udp 546/udp 53/tcp 53/udp 80/tcp 443/tcp 22/tcp 69/udp","title":"Preflight Checks"},{"location":"preflight/#preflight","text":"The preflight subcommand checks for port conflicts, systemd/service conflicts, and firewall rules on the host and can optionally fix errors it finds. Example: helpernodectl preflight This will only report conflicts/errors and leaves it up to the user to fix/reconcile. You can optionally fix systemd and firewall rules by passing the --fix-all flag (EXPERIMENTAL). Example: helpernodectl preflight --fix-all Again, this is experimental and assumes you have firewalld running on a RHEL 8/CentOS 8 server/node/machine/vm. Currently the firewall rules needed for the HelperNode are: 6443/tcp 22623/tcp 8080/tcp 9000/tcp 9090/tcp 67/udp 546/udp 53/tcp 53/udp 80/tcp 443/tcp 22/tcp 69/udp","title":"Preflight"},{"location":"pull/","text":"Pull The pull subcommand pulls the default \"core\" images onto the node and sets up initial ~/.helpernodectl.yaml config file. Example: helpernodectl pull Currently, the default \"core\" images being pulled are: quay.io/helpernode/pxe quay.io/helpernode/http quay.io/helpernode/loadbalancer quay.io/helpernode/dns quay.io/helpernode/dhcp Disconnected Pulling If you are running the HelperNode in a disconnected environment (and have pulled/tagged/pushed the core images into your registry), you can use the HELPERNODE_IMAGE_PREFIX environment variable to indicate what registry plus prefix you'd like to use. Example: export HELPERNODE_IMAGE_PREFIX=registry.example.com:5000/mystuff This will result in the images having the prefix of registry.example.com:5000/mystuff and suffix of /helpernode/<service> . For example with the HELPERNODE_IMAGE_PREFIX set to registry.example.com:5000/mystuff the helpernodectl pull command will try and pull the following: registry.example.com:5000/mystuff/helpernode/pxe registry.example.com:5000/mystuff/helpernode/http registry.example.com:5000/mystuff/helpernode/loadbalancer registry.example.com:5000/mystuff/helpernode/dns registry.example.com:5000/mystuff/helpernode/dhcp","title":"Pulling Images"},{"location":"pull/#pull","text":"The pull subcommand pulls the default \"core\" images onto the node and sets up initial ~/.helpernodectl.yaml config file. Example: helpernodectl pull Currently, the default \"core\" images being pulled are: quay.io/helpernode/pxe quay.io/helpernode/http quay.io/helpernode/loadbalancer quay.io/helpernode/dns quay.io/helpernode/dhcp","title":"Pull"},{"location":"pull/#disconnected-pulling","text":"If you are running the HelperNode in a disconnected environment (and have pulled/tagged/pushed the core images into your registry), you can use the HELPERNODE_IMAGE_PREFIX environment variable to indicate what registry plus prefix you'd like to use. Example: export HELPERNODE_IMAGE_PREFIX=registry.example.com:5000/mystuff This will result in the images having the prefix of registry.example.com:5000/mystuff and suffix of /helpernode/<service> . For example with the HELPERNODE_IMAGE_PREFIX set to registry.example.com:5000/mystuff the helpernodectl pull command will try and pull the following: registry.example.com:5000/mystuff/helpernode/pxe registry.example.com:5000/mystuff/helpernode/http registry.example.com:5000/mystuff/helpernode/loadbalancer registry.example.com:5000/mystuff/helpernode/dns registry.example.com:5000/mystuff/helpernode/dhcp","title":"Disconnected Pulling"},{"location":"sample-yamls/","text":"Sample YAML Configurations Sample configuration files can be found here. These are samples of most common use cases. Full Stack A \"fullstack\" install of the HelperNode services. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda Static IPs Here is an example if you're doing a \"static ip\" install and don't need all the services. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" disabledServices: - dhcp - pxe dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" - name: \"master1\" ipaddr: \"192.168.7.22\" - name: \"master2\" ipaddr: \"192.168.7.23\" workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" - name: \"worker1\" ipaddr: \"192.168.7.12\" Compact Static Here is an example of setting up a \"compact\" cluster (where you have 3 nodes that act as a master and a worker), not using pxe or dhcp since a static ip install will be performed. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" disabledServices: - dhcp - pxe dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" - name: \"master1\" ipaddr: \"192.168.7.22\" - name: \"master2\" ipaddr: \"192.168.7.23\" Pluggable Services A \"fullstack\" install of the HelperNode services with Pluggable Services. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda pluggableServices: anotherweb: image: quay.io/christianh814/test-webserver:latest ports: - 8899/tcp - 8899/udp myweb: image: quay.io/christianh814/webserver-test:latest ports: - 8888/tcp","title":"Sample YAML Configurations"},{"location":"sample-yamls/#sample-yaml-configurations","text":"Sample configuration files can be found here. These are samples of most common use cases.","title":"Sample YAML Configurations"},{"location":"sample-yamls/#full-stack","text":"A \"fullstack\" install of the HelperNode services. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda","title":"Full Stack"},{"location":"sample-yamls/#static-ips","text":"Here is an example if you're doing a \"static ip\" install and don't need all the services. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" disabledServices: - dhcp - pxe dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" - name: \"master1\" ipaddr: \"192.168.7.22\" - name: \"master2\" ipaddr: \"192.168.7.23\" workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" - name: \"worker1\" ipaddr: \"192.168.7.12\"","title":"Static IPs"},{"location":"sample-yamls/#compact-static","text":"Here is an example of setting up a \"compact\" cluster (where you have 3 nodes that act as a master and a worker), not using pxe or dhcp since a static ip install will be performed. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" disabledServices: - dhcp - pxe dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" masters: - name: \"master0\" ipaddr: \"192.168.7.21\" - name: \"master1\" ipaddr: \"192.168.7.22\" - name: \"master2\" ipaddr: \"192.168.7.23\"","title":"Compact Static"},{"location":"sample-yamls/#pluggable-services","text":"A \"fullstack\" install of the HelperNode services with Pluggable Services. version: v2 arch: \"x86_64\" helper: name: \"helper\" ipaddr: \"192.168.7.77\" networkifacename: \"ens3\" dns: domain: \"example.com\" clusterid: \"ocp4\" forwarder1: \"8.8.8.8\" forwarder2: \"8.8.4.4\" dhcp: router: \"192.168.7.1\" bcast: \"192.168.7.255\" netmask: \"255.255.255.0\" poolstart: \"192.168.7.10\" poolend: \"192.168.7.30\" ipid: \"192.168.7.0\" netmaskid: \"255.255.255.0\" bootstrap: name: \"bootstrap\" ipaddr: \"192.168.7.20\" macaddr: \"52:54:00:60:72:67\" disk: vda masters: - name: \"master0\" ipaddr: \"192.168.7.21\" macaddr: \"52:54:00:e7:9d:67\" disk: vda - name: \"master1\" ipaddr: \"192.168.7.22\" macaddr: \"52:54:00:80:16:23\" disk: vda - name: \"master2\" ipaddr: \"192.168.7.23\" macaddr: \"52:54:00:d5:1c:39\" disk: vda workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda pluggableServices: anotherweb: image: quay.io/christianh814/test-webserver:latest ports: - 8899/tcp - 8899/udp myweb: image: quay.io/christianh814/webserver-test:latest ports: - 8888/tcp","title":"Pluggable Services"},{"location":"saving-yaml/","text":"Saving YAML Config The save subcommand is used to save a configuration file needed to start the HelperNode services. Once saved, you no longer have to pass --config to the start subcommand. Example: helpernodectl save -f helpernode.yaml This will save the provided config file to ~/.helper.yaml","title":"Saving YAML Config"},{"location":"saving-yaml/#saving-yaml-config","text":"The save subcommand is used to save a configuration file needed to start the HelperNode services. Once saved, you no longer have to pass --config to the start subcommand. Example: helpernodectl save -f helpernode.yaml This will save the provided config file to ~/.helper.yaml","title":"Saving YAML Config"},{"location":"shell-completion/","text":"Shell Completion Shell completion generates a shell completion script for your shell environment. This assumes you have your shell compeltion package installed ( bash-completion on most RHEL/CentOS systems) Example: source <(helpernodectl completion bash) Usage: helpernodectl completion [bash|zsh|fish|powershell] Currently, only the following shells are supported bash zsh fish powershell","title":"Shell Completion"},{"location":"shell-completion/#shell-completion","text":"Shell completion generates a shell completion script for your shell environment. This assumes you have your shell compeltion package installed ( bash-completion on most RHEL/CentOS systems) Example: source <(helpernodectl completion bash) Usage: helpernodectl completion [bash|zsh|fish|powershell] Currently, only the following shells are supported bash zsh fish powershell","title":"Shell Completion"},{"location":"start/","text":"Starting the HelperNode Services The start subcommand will start the containers needed for the HelperNode to run. It will run, and configure, the services depending on what YAML configuration is passed. Example: helpernodectl start --config helpernode.yaml The --config needs to be passed, unless you performed a save of the config file (which saves it under ~/.helper.yaml ). Preflight Checks A preflight check is performed when you issue the start command. This ensures that there is no conflicts before starting the services. This will sometimes produce a \"false positive\" when making changes and wanting to restart the services. To skip the preflight steps during startup by passing --skip-preflight or the shorthand -s . Example: helpernodectl start --config helpernode.yaml --skip-preflight This will start the services without checking for errors first. Use with caution. Starting Individual Services By default, start will start all \"core\" services. You can start individua services if, for example, you stopped the http container to make a config change in your YAML file; and need to start it up again. Example: helpernodectl start --config helpernode.yaml http You can start multiple services by using a comma ( , ) as a delimiter. Example: helpernodectl start --config helpernode.yaml http,pxe The same caveat of preflight checks applies. You can pass --skip-preflight (or the -s shorthand) to skip these checks. Example: helpernodectl start --config helpernode.yaml dhcp,pxe --skip-preflight","title":"Starting the HelperNode Services"},{"location":"start/#starting-the-helpernode-services","text":"The start subcommand will start the containers needed for the HelperNode to run. It will run, and configure, the services depending on what YAML configuration is passed. Example: helpernodectl start --config helpernode.yaml The --config needs to be passed, unless you performed a save of the config file (which saves it under ~/.helper.yaml ).","title":"Starting the HelperNode Services"},{"location":"start/#preflight-checks","text":"A preflight check is performed when you issue the start command. This ensures that there is no conflicts before starting the services. This will sometimes produce a \"false positive\" when making changes and wanting to restart the services. To skip the preflight steps during startup by passing --skip-preflight or the shorthand -s . Example: helpernodectl start --config helpernode.yaml --skip-preflight This will start the services without checking for errors first. Use with caution.","title":"Preflight Checks"},{"location":"start/#starting-individual-services","text":"By default, start will start all \"core\" services. You can start individua services if, for example, you stopped the http container to make a config change in your YAML file; and need to start it up again. Example: helpernodectl start --config helpernode.yaml http You can start multiple services by using a comma ( , ) as a delimiter. Example: helpernodectl start --config helpernode.yaml http,pxe The same caveat of preflight checks applies. You can pass --skip-preflight (or the -s shorthand) to skip these checks. Example: helpernodectl start --config helpernode.yaml dhcp,pxe --skip-preflight","title":"Starting Individual Services"},{"location":"status/","text":"Displaying Status The status subcommand shows the status of the running containers on the host. Example: helpernodectl status Sample Output: Names Status Image helpernode-pxe Up 46 minutes ago registry.example.com:5000/alpha/helpernode/pxe:latest helpernode-loadbalancer Up 46 minutes ago registry.example.com:5000/alpha/helpernode/loadbalancer:latest helpernode-http Up 46 minutes ago registry.example.com:5000/alpha/helpernode/http:latest helpernode-dhcp Up 46 minutes ago registry.example.com:5000/alpha/helpernode/dhcp:latest helpernode-dns Up 46 minutes ago registry.example.com:5000/alpha/helpernode/dns:latest This simply passes you the information provided by the contianer runtime.","title":"Displaying Status"},{"location":"status/#displaying-status","text":"The status subcommand shows the status of the running containers on the host. Example: helpernodectl status Sample Output: Names Status Image helpernode-pxe Up 46 minutes ago registry.example.com:5000/alpha/helpernode/pxe:latest helpernode-loadbalancer Up 46 minutes ago registry.example.com:5000/alpha/helpernode/loadbalancer:latest helpernode-http Up 46 minutes ago registry.example.com:5000/alpha/helpernode/http:latest helpernode-dhcp Up 46 minutes ago registry.example.com:5000/alpha/helpernode/dhcp:latest helpernode-dns Up 46 minutes ago registry.example.com:5000/alpha/helpernode/dns:latest This simply passes you the information provided by the contianer runtime.","title":"Displaying Status"},{"location":"stop/","text":"Stopping the HelperNode Services The stop subcommand will stop the HelperNode containers currently running. Example: helpernodectl stop Stopping Individual Services By default, stop will stop all \"core\" services. You can stop individual services if, for example, you want the http container to be shutdown since you don't need it post install. Example: helpernodectl stop http You can stop multiple services by using a comma ( , ) as a delimiter. Example: helpernodectl stop dhcp,pxe","title":"Stopping the HelperNode Services"},{"location":"stop/#stopping-the-helpernode-services","text":"The stop subcommand will stop the HelperNode containers currently running. Example: helpernodectl stop","title":"Stopping the HelperNode Services"},{"location":"stop/#stopping-individual-services","text":"By default, stop will stop all \"core\" services. You can stop individual services if, for example, you want the http container to be shutdown since you don't need it post install. Example: helpernodectl stop http You can stop multiple services by using a comma ( , ) as a delimiter. Example: helpernodectl stop dhcp,pxe","title":"Stopping Individual Services"},{"location":"worker-section/","text":"Worker Nodes Section Similar to the master section; this sets up worker node configuration. Please note that this is an array. \ud83d\udea8 This section is optional if you're installing a compact cluster. workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda workers.name - The hostname (WITHOUT the fqdn) of the worker node you want to set workers.ipaddr - The IP address that you want set (this modifies the dhcp config file, the dns zonefile, and the reverse dns zonefile) workers.macaddr - The mac address for dhcp reservation. This option is not needed if you're doing static ips. workers.disk - The name of the disk to install RHCOS onto. NOTE: At LEAST 2 workers is needed if you're installing a standard version of OpenShift 4","title":"Worker Nodes Section"},{"location":"worker-section/#worker-nodes-section","text":"Similar to the master section; this sets up worker node configuration. Please note that this is an array. \ud83d\udea8 This section is optional if you're installing a compact cluster. workers: - name: \"worker0\" ipaddr: \"192.168.7.11\" macaddr: \"52:54:00:f4:26:a1\" disk: vda - name: \"worker1\" ipaddr: \"192.168.7.12\" macaddr: \"52:54:00:82:90:00\" disk: vda workers.name - The hostname (WITHOUT the fqdn) of the worker node you want to set workers.ipaddr - The IP address that you want set (this modifies the dhcp config file, the dns zonefile, and the reverse dns zonefile) workers.macaddr - The mac address for dhcp reservation. This option is not needed if you're doing static ips. workers.disk - The name of the disk to install RHCOS onto. NOTE: At LEAST 2 workers is needed if you're installing a standard version of OpenShift 4","title":"Worker Nodes Section"},{"location":"yaml-file-forward/","text":"The helpernode.yaml File The configuration file for the HelperNode is a YAML file that can be named anything, but will be referred to in this documentation as helpernode.yaml . This configuration file is used as the \"source of truth\" to configure and start the services needed to run the HelperNode. This config file needs to be either saved (by using the save subcommand) or passed to the start command. helpernodectl start --config helpernode.yaml When each service starts, the helpernode.yaml file is passed to the container and each individual container uses this YAML file to configure and start the services. NOTE: Currently, no validation is done by the container. It's \"garbage in/garbage out\" in it's current form. Versioning and Architecture The current version of the YAML manifest is v2 . Only x86_64 is currently supported. version: v2 arch: \"x86_64\" Plans are in place to support PPCLE and ARM . ARM support will come at after OpenShift supports ARM.","title":"The helpernode.yaml File"},{"location":"yaml-file-forward/#the-helpernodeyaml-file","text":"The configuration file for the HelperNode is a YAML file that can be named anything, but will be referred to in this documentation as helpernode.yaml . This configuration file is used as the \"source of truth\" to configure and start the services needed to run the HelperNode. This config file needs to be either saved (by using the save subcommand) or passed to the start command. helpernodectl start --config helpernode.yaml When each service starts, the helpernode.yaml file is passed to the container and each individual container uses this YAML file to configure and start the services. NOTE: Currently, no validation is done by the container. It's \"garbage in/garbage out\" in it's current form.","title":"The helpernode.yaml File"},{"location":"yaml-file-forward/#versioning-and-architecture","text":"The current version of the YAML manifest is v2 . Only x86_64 is currently supported. version: v2 arch: \"x86_64\" Plans are in place to support PPCLE and ARM . ARM support will come at after OpenShift supports ARM.","title":"Versioning and Architecture"}]}